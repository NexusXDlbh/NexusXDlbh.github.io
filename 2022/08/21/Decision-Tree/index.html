<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Decision Tree | Meta Field</title><meta name="author" content="NexusLbh"><meta name="copyright" content="NexusLbh"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="日拱一卒无有尽，功不唐捐终入海">
<meta property="og:type" content="article">
<meta property="og:title" content="Decision Tree">
<meta property="og:url" content="http://nexuslbh.top/2022/08/21/Decision-Tree/index.html">
<meta property="og:site_name" content="Meta Field">
<meta property="og:description" content="日拱一卒无有尽，功不唐捐终入海">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://nexuslbh.top/img/men.jpg">
<meta property="article:published_time" content="2022-08-21T12:25:11.000Z">
<meta property="article:modified_time" content="2023-04-03T08:02:34.442Z">
<meta property="article:author" content="NexusLbh">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://nexuslbh.top/img/men.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://nexuslbh.top/2022/08/21/Decision-Tree/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Decision Tree',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-04-03 16:02:34'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/font.css"><meta name="generator" content="Hexo 5.4.2"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">67</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/men.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Meta Field"><span class="site-name">Meta Field</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Decision Tree</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-08-21T12:25:11.000Z" title="发表于 2022-08-21 20:25:11">2022-08-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-04-03T08:02:34.442Z" title="更新于 2023-04-03 16:02:34">2023-04-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/MachineLearning/">MachineLearning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Decision Tree"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><strong>日拱一卒无有尽，功不唐捐终入海</strong></p>
<span id="more"></span>
<h2 id="Basic-Theories"><a href="#Basic-Theories" class="headerlink" title="Basic Theories"></a>Basic Theories</h2><h3 id="1-Algorithm-Priciple"><a href="#1-Algorithm-Priciple" class="headerlink" title="1.Algorithm Priciple"></a>1.Algorithm Priciple</h3><p><strong>分类，决策树每个内部结点</strong>表示在一个属性上的一个测试，每个<strong>分支</strong>代表一个测试输出，每个<strong>叶节点</strong>代表一种类别。</p>
<p><strong>Advantages:</strong></p>
<p><strong>①自学习</strong></p>
<p><strong>②可读性好</strong></p>
<p><strong>③效率高</strong></p>
<p><img src="/2022/08/21/Decision-Tree/dtree.png" alt></p>
<h3 id="2-Information-Entropy-amp-Information-Gain"><a href="#2-Information-Entropy-amp-Information-Gain" class="headerlink" title="2.Information Entropy & Information Gain"></a>2.Information Entropy &amp; Information Gain</h3><p><strong>Shannon</strong>,the father of information theory,proposed the concept of <strong>“information entropy”.</strong></p>
<p>在决策树分类中，随着划分过程不断进行，我们希望决策树分支结点所包含的样本尽可能属于同一类别，即：结点纯度(purity)越来越高。</p>
<p><strong>信息论里面的信息与我们平时理解的不一样：</strong></p>
<p><strong>信息是消除随机不定性的事物。</strong></p>
<p><strong>小明今年18岁（√）</strong></p>
<p>小明明年19岁（×）~也就是废话~</p>
<p><strong>量化信息大小：信息熵。是描述消息中，不确定性</strong>的值，<strong>熵越高，不确定性越高,可以理解为“混合的数据越多”。</strong></p>
<p><strong>假定当前样本集合</strong>D<strong>中第</strong>k<strong>类样本所占比例为</strong>p_k(k=1,2,…,|y|),<strong>则信息熵定义为：</strong></p>
<script type="math/tex; mode=display">
Ent(D)=-\sum_{k=1}^{|y|} p_k \log_2p_k \quad\quad (*)</script><p><strong>假定离散属性</strong>a<strong>有</strong>V<strong>个可能的取值</strong><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="13.031ex" height="2.353ex" role="img" focusable="false" viewbox="0 -846 5759.5 1040"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mn" transform="translate(562,363) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g><g data-mml-node="mo" transform="translate(965.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msup" transform="translate(1410.2,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mn" transform="translate(562,363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g><g data-mml-node="mo" transform="translate(2375.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mo" transform="translate(2820.4,0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"/></g><g data-mml-node="mo" transform="translate(4159.1,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="msup" transform="translate(4603.8,0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mi" transform="translate(562,363) scale(0.707)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"/></g></g></g></g></g></svg></mjx-container>，若使用<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewbox="0 -441 529 451"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g></g></g></svg></mjx-container>来对样本进行划分，则产生<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewbox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"/></g></g></g></svg></mjx-container>个分支结点，其中第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.097ex" height="1.027ex" role="img" focusable="false" viewbox="0 -443 485 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g></g></g></svg></mjx-container>个分支结点包含了<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.873ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 828 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g></g></svg></mjx-container>中所有在属性<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewbox="0 -441 529 451"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g></g></g></svg></mjx-container>上取值为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="2.161ex" height="1.553ex" role="img" focusable="false" viewbox="0 -676.2 954.9 686.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mi" transform="translate(562,363) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g></g></g></g></svg></mjx-container>,记为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.837ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 1253.9 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g><g data-mml-node="mi" transform="translate(861,363) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g></g></g></g></svg></mjx-container>，我们可以根据*式计算出<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="2.837ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 1253.9 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g><g data-mml-node="mi" transform="translate(861,363) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g></g></g></g></svg></mjx-container>的信息熵。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">def</span> <span class="title function_">cal_Ent</span>(<span class="params">dataSet</span>):</span><br><span class="line">     n = dataSet.shape[<span class="number">0</span>] <span class="comment">#数据集总行数</span></span><br><span class="line">     iset = dataSet.iloc[:,-<span class="number">1</span>].value_counts() <span class="comment">#标签的所有类别</span></span><br><span class="line">     p = iset/n <span class="comment">#每一类标签所占比</span></span><br><span class="line">     ent = (-p*np.log2(p)).<span class="built_in">sum</span>() <span class="comment">#计算信息熵</span></span><br><span class="line">     <span class="keyword">return</span> ent </span><br></pre></td></tr></table></figure>
<p><strong>考虑到不同分支结点所包含的样本数不同，给分支结点赋予权重<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="8.357ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 3693.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo" transform="translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="msup" transform="translate(278,0)"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g><g data-mml-node="mi" transform="translate(861,363) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g></g><g data-mml-node="mo" transform="translate(1531.9,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1809.9,0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"/></g></g><g data-mml-node="mo" transform="translate(2309.9,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g><g data-mml-node="mi" transform="translate(2587.9,0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g><g data-mml-node="mo" transform="translate(3415.9,0) translate(0 -0.5)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"/></g></g></g></svg></mjx-container>，即样本数越多，分支结点影响越大，于是可以计算出用属性</strong><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.197ex" height="1.02ex" role="img" focusable="false" viewbox="0 -441 529 451"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g></g></g></svg></mjx-container><strong>对样本集</strong><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.873ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 828 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"/></g></g></g></svg></mjx-container>进行划分所获得的“信息增益”。信息增益越大，表示：使用属性a来进行划分所获得的“纯度提升”越大。信息增益是决策树划分依据之一。</p>
<p><strong>计算：信息增益 = 总的信息熵 — 知道某个信息后的信息熵</strong></p>
<script type="math/tex; mode=display">
Gain(D,a) = Ent(D) - \sum_{v=1}^{V} \frac{|D^v|}{|D|} Ent(D^v)</script><p><strong>条件熵计算</strong></p>
<script type="math/tex; mode=display">
Ent(D|A) = \sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)</script><p><strong>eg:</strong></p>
<script type="math/tex; mode=display">
Ent(D|年龄) = Ent(青年)+Ent(中年)+Ent(老年)</script><h2 id="Algorithm-Implementation-in-Python"><a href="#Algorithm-Implementation-in-Python" class="headerlink" title="Algorithm Implementation in Python"></a>Algorithm Implementation in Python</h2><h3 id="1-Core-Problems"><a href="#1-Core-Problems" class="headerlink" title="1.Core Problems"></a>1.Core Problems</h3><p><strong>①如何从数据表中找到最佳节点和最佳分枝？</strong></p>
<p><strong>得到原始数据集，然后基于最好的属性值划分数据集。</strong></p>
<p><strong>②如何让决策树停止生长，防止过拟合？</strong></p>
<p><strong>过拟合判断：当训练集和测试集的准确率相差很大时（例如：训练集1.0，测试集0.8）,可以认为模型过拟合。</strong></p>
<p>过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化，也就是常说的<strong>剪枝处理。</strong></p>
<h3 id="2-Implementation"><a href="#2-Implementation" class="headerlink" title="2.Implementation"></a>2.Implementation</h3><p><strong>这里使用ID3算法实现</strong></p>
<p><img src="/2022/08/21/Decision-Tree/dtree1.png" alt></p>
<h4 id="1-特征选择"><a href="#1-特征选择" class="headerlink" title="(1)特征选择"></a>(1)特征选择</h4><p><strong>①计算信息熵和信息增益</strong></p>
<p><strong>②数据集最佳切分函数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"> <span class="string">"""</span></span><br><span class="line"><span class="string"> 函数功能：根据信息增益选择出最佳数据集切分的列</span></span><br><span class="line"><span class="string"> 参数说明：</span></span><br><span class="line"><span class="string"> dataSet：原始数据集</span></span><br><span class="line"><span class="string"> 返回：</span></span><br><span class="line"><span class="string"> axis:数据集最佳切分列的索引</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"> </span><br><span class="line"> <span class="comment">#选择最优的列进行切分</span></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">bestSplit</span>(<span class="params">dataSet</span>):</span><br><span class="line">     baseEnt = calEnt(dataSet)<span class="comment">#计算原始熵</span></span><br><span class="line">     bestGain = <span class="number">0</span>             <span class="comment">#初始化信息增益</span></span><br><span class="line">     axis = -<span class="number">1</span>                <span class="comment">#初始化最佳切分列，标签列</span></span><br><span class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(dataSet.shape[<span class="number">1</span>]-<span class="number">1</span>): <span class="comment">#对特征的每一列进行循环</span></span><br><span class="line">         levels= dataSet.iloc[:,i].value_counts().index   </span><br><span class="line">   <span class="comment">#提取出当前列的所有取值</span></span><br><span class="line">         ents = <span class="number">0</span></span><br><span class="line"><span class="comment">#初始化子节点的信息熵       </span></span><br><span class="line">         <span class="keyword">for</span> j <span class="keyword">in</span> levels: </span><br><span class="line">   <span class="comment">#对当前列的每一个取值进行循环</span></span><br><span class="line">             childSet = dataSet[dataSet.iloc[:,i]==j]  </span><br><span class="line">       <span class="comment">#某一个子节点的dataframe</span></span><br><span class="line">             ent = calEnt(childSet)						 </span><br><span class="line">   <span class="comment">#计算某一个子节点的信息熵</span></span><br><span class="line">             ents += (childSet.shape[<span class="number">0</span>]/dataSet.shape[<span class="number">0</span>])*ent </span><br><span class="line">       <span class="comment">#计算当前列的信息熵</span></span><br><span class="line">         <span class="comment">#print(f'第{i}列的信息熵为{ents}')</span></span><br><span class="line">         infoGain = baseEnt-ents <span class="comment">#计算当前列的信息增益</span></span><br><span class="line">         <span class="comment">#print(f'第{i}列的信息增益为{infoGain}')</span></span><br><span class="line">         <span class="keyword">if</span> (infoGain &gt; bestGain):</span><br><span class="line">             bestGain = infoGain       </span><br><span class="line">   <span class="comment">#选择最大信息增益</span></span><br><span class="line">             axis = i    </span><br><span class="line">   <span class="comment">#最大信息增益所在列的索引</span></span><br><span class="line">     <span class="keyword">return</span> axis</span><br></pre></td></tr></table></figure>
<p><strong>③按照给定列切分数据集</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> <span class="string">"""</span></span><br><span class="line"><span class="string"> 函数功能：按照给定的列划分数据集</span></span><br><span class="line"><span class="string"> 参数说明：</span></span><br><span class="line"><span class="string"> dataSet：原始数据集</span></span><br><span class="line"><span class="string"> axis：指定的列索引</span></span><br><span class="line"><span class="string"> value：指定的属性值</span></span><br><span class="line"><span class="string"> 返回：</span></span><br><span class="line"><span class="string"> redataSet：按照指定列索引和属性值切分后的数据集</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"> </span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">mySplit</span>(<span class="params">dataSet,axis,value</span>):</span><br><span class="line">     col = dataSet.columns[axis]</span><br><span class="line">     redataSet = dataSet.loc[dataSet[col]==value,:].drop(col,axis=<span class="number">1</span>)</span><br><span class="line">     <span class="keyword">return</span> redataSet  </span><br></pre></td></tr></table></figure>
<h4 id="2-决策树生成（构建决策树）"><a href="#2-决策树生成（构建决策树）" class="headerlink" title="(2)决策树生成（构建决策树）"></a>(2)决策树生成（构建决策树）</h4><p><strong>①开始，所有记录看作一个节点</strong></p>
<p><strong>②遍历每个特征的每一种分裂方式，找到最好的分裂特征（分裂点）</strong></p>
<p><strong>③分裂成两个或多个节点</strong></p>
<p><strong>④对分裂后的节点分别继续执行2-3步，直到每个节点足够“纯”为止</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"> <span class="string">"""</span></span><br><span class="line"><span class="string"> 函数功能：基于最大信息增益切分数据集，递归构建决策树</span></span><br><span class="line"><span class="string"> 参数说明：</span></span><br><span class="line"><span class="string"> dataSet：原始数据集（最后一列是标签）</span></span><br><span class="line"><span class="string"> 返回：</span></span><br><span class="line"><span class="string"> myTree：字典形式的树</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">dataSet</span>):</span><br><span class="line">     featlist = <span class="built_in">list</span>(dataSet.columns)       </span><br><span class="line"><span class="comment">#提取出数据集所有的列</span></span><br><span class="line">     classlist = dataSet.iloc[:,-<span class="number">1</span>].value_counts() </span><br><span class="line"><span class="comment">#获取最后一列类标签</span></span><br><span class="line">     <span class="comment">#判断最多标签数目是否等于数据集行数，或者数据集是否只有一列</span></span><br><span class="line">     <span class="keyword">if</span> classlist[<span class="number">0</span>]==dataSet.shape[<span class="number">0</span>] <span class="keyword">or</span> dataSet.shape[<span class="number">1</span>] == <span class="number">1</span>:</span><br><span class="line">         <span class="keyword">return</span> classlist.index[<span class="number">0</span>]                  </span><br><span class="line"><span class="comment">#如果是，返回类标签</span></span><br><span class="line">     axis = bestSplit(dataSet)      </span><br><span class="line"><span class="comment">#确定出当前最佳切分列的索引</span></span><br><span class="line">     bestfeat = featlist[axis]    </span><br><span class="line"><span class="comment">#获取该索引对应的特征</span></span><br><span class="line">     myTree = {bestfeat:{}}    </span><br><span class="line"><span class="comment">#采用字典嵌套的方式存储树信息</span></span><br><span class="line">     <span class="keyword">del</span> featlist[axis]</span><br><span class="line"><span class="comment">#删除当前特征</span></span><br><span class="line">     valuelist = <span class="built_in">set</span>(dataSet.iloc[:,axis])   </span><br><span class="line"><span class="comment">#提取最佳切分列所有属性值</span></span><br><span class="line">     <span class="keyword">for</span> value <span class="keyword">in</span> valuelist:   </span><br><span class="line"><span class="comment">#对每一个属性值递归建树</span></span><br><span class="line">         myTree[bestfeat][value] = createTree(mySplit(dataSet,axis,value))</span><br><span class="line">     <span class="keyword">return</span> myTree</span><br></pre></td></tr></table></figure>
<h4 id="3-决策树存储"><a href="#3-决策树存储" class="headerlink" title="(3)决策树存储"></a>(3)决策树存储</h4><p><strong>构造决策树是很耗时的任务，即使处理很小的数据集，也要花费几秒的时间，如果数据集很大，将会耗费很多计算时间。因此为了节省时间，建好树之后立马将其保存，后续使用直接调用即可。</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#树的存储</span></span><br><span class="line"> np.save(<span class="string">'myTree.npy'</span>,myTree)</span><br><span class="line"> </span><br><span class="line"> <span class="comment">#直接把字典形式的数据保存为.npy文件</span></span><br><span class="line"> <span class="comment">#直接用load()函数读取即可</span></span><br><span class="line"> </span><br><span class="line"> <span class="comment">#树的读取</span></span><br><span class="line"> read_myTree = np.load(<span class="string">'myTree.npy'</span>).item()   </span><br><span class="line"> read_myTree</span><br></pre></td></tr></table></figure>
<h4 id="4-决策树分类"><a href="#4-决策树分类" class="headerlink" title="(4)决策树分类"></a>(4)决策树分类</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"> <span class="string">"""</span></span><br><span class="line"><span class="string"> 函数功能：对一个测试实例进行分类</span></span><br><span class="line"><span class="string"> 参数说明：</span></span><br><span class="line"><span class="string"> inputTree：已经生成的决策树</span></span><br><span class="line"><span class="string"> labels：存储选择的最优特征标签</span></span><br><span class="line"><span class="string"> testVec：测试数据列表，顺序对应原数据集</span></span><br><span class="line"><span class="string"> 返回：</span></span><br><span class="line"><span class="string"> classLabel：分类结果</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">inputTree,labels, testVec</span>):</span><br><span class="line">     firstStr = <span class="built_in">next</span>(<span class="built_in">iter</span>(inputTree))                   </span><br><span class="line"><span class="comment">#获取决策树第一个节点</span></span><br><span class="line">     secondDict = inputTree[firstStr]                   </span><br><span class="line"><span class="comment">#下一个字典</span></span><br><span class="line">     featIndex = labels.index(firstStr)        </span><br><span class="line"><span class="comment">#第一个节点所在列的索引</span></span><br><span class="line">     <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():</span><br><span class="line">         <span class="keyword">if</span> testVec[featIndex] == key:</span><br><span class="line">             <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[key]) == <span class="built_in">dict</span> :</span><br><span class="line">                 classLabel = classify(secondDict[key], labels, testVec)</span><br><span class="line">             <span class="keyword">else</span>: </span><br><span class="line">                 classLabel = secondDict[key]</span><br><span class="line">     <span class="keyword">return</span> classLabel</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"> <span class="string">"""</span></span><br><span class="line"><span class="string"> 函数功能：对测试集进行预测，并返回预测后的结果</span></span><br><span class="line"><span class="string"> 参数说明：</span></span><br><span class="line"><span class="string"> train：训练集</span></span><br><span class="line"><span class="string"> test：测试集</span></span><br><span class="line"><span class="string"> 返回：</span></span><br><span class="line"><span class="string"> test：预测好分类的测试集</span></span><br><span class="line"><span class="string"> """</span></span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">acc_classify</span>(<span class="params">train,test</span>):</span><br><span class="line">     inputTree = createTree(train)<span class="comment">#根据测试集生成一棵树</span></span><br><span class="line">     labels = <span class="built_in">list</span>(train.columns)<span class="comment">#数据集所有的列名称</span></span><br><span class="line">     result = []</span><br><span class="line">     <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(test.shape[<span class="number">0</span>]):<span class="comment">#对测试集中每一条数据进行循环</span></span><br><span class="line">         testVec = test.iloc[i,:-<span class="number">1</span>]<span class="comment">#测试集中的一个实例</span></span><br><span class="line">         classLabel = classify(inputTree,labels,testVec)<span class="comment">#预测该实例的分类</span></span><br><span class="line">         result.append(classLabel)<span class="comment">#将分类结果追加到result列表中</span></span><br><span class="line">     test[<span class="string">'predict'</span>]=result<span class="comment">#将预测结果追加到测试集最后一列</span></span><br><span class="line">     acc = (test.iloc[:,-<span class="number">1</span>]==test.iloc[:,-<span class="number">2</span>]).mean()<span class="comment">#计算准确率</span></span><br><span class="line">     <span class="built_in">print</span>(<span class="string">f'模型预测准确率为<span class="subst">{acc}</span>'</span>)</span><br><span class="line">     <span class="keyword">return</span> test </span><br><span class="line"> </span><br></pre></td></tr></table></figure>
<p><strong>测试函数</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> train = dataSet</span><br><span class="line"> test = dataSet.iloc[:<span class="number">3</span>,:]</span><br><span class="line"> acc_classify(train,test)</span><br></pre></td></tr></table></figure>
<h4 id="5-决策树绘制"><a href="#5-决策树绘制" class="headerlink" title="(5)决策树绘制"></a>(5)决策树绘制</h4><p><strong>使用SKlearn中graphviz包</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入相应的包</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">import</span> graphviz</span><br><span class="line"></span><br><span class="line"><span class="comment">#特征</span></span><br><span class="line">Xtrain = dataSet.iloc[:,:-<span class="number">1</span>]</span><br><span class="line"><span class="comment">#标签</span></span><br><span class="line">Ytrain = dataSet.iloc[:,-<span class="number">1</span>]</span><br><span class="line">labels = Ytrain.unique().tolist()</span><br><span class="line"><span class="comment">#将本文转换为数字</span></span><br><span class="line">Ytrain = Ytrain.apply(<span class="keyword">lambda</span> x: labels.index(x))  </span><br><span class="line"></span><br><span class="line"><span class="comment">#绘制树模型</span></span><br><span class="line">clf = DecisionTreeClassifier()</span><br><span class="line">clf = clf.fit(Xtrain, Ytrain)</span><br><span class="line">tree.export_graphviz(clf)</span><br><span class="line">dot_data = tree.export_graphviz(clf, out_file=<span class="literal">None</span>)</span><br><span class="line">graphviz.Source(dot_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">#给图形增加标签和颜色</span></span><br><span class="line">dot_data = tree.export_graphviz(clf, out_file=<span class="literal">None</span>,</span><br><span class="line">                                feature_names=[<span class="string">'no surfacing'</span>,<span class="string">'flippers'</span>],</span><br><span class="line">                                class_names=[<span class="string">'fish'</span>, <span class="string">'not fish'</span>],</span><br><span class="line">                                filled=<span class="literal">True</span>, rounded=<span class="literal">True</span>,</span><br><span class="line">                                special_characters=<span class="literal">True</span>)</span><br><span class="line">graphviz.Source(dot_data)</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用render方法生成图形</span></span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line">graph.render(<span class="string">"fish"</span>)</span><br></pre></td></tr></table></figure>
<h4 id="6-决策树剪枝（了解）"><a href="#6-决策树剪枝（了解）" class="headerlink" title="(6)决策树剪枝（了解）"></a>(6)决策树剪枝（了解）</h4><p><strong>将决策树的某些内部节点下面的节点都删掉，留下来的内部决策节点作为叶子节点。</strong></p>
<p>有两种剪枝策略，分别是预剪枝和后剪枝。</p>
<p><strong>①</strong>预剪枝（pre-pruning）</p>
<p><strong>构造时候就考虑剪枝</strong></p>
<p><strong>②</strong>后剪枝（post-pruning）</p>
<p><strong>构造完成再剪枝</strong></p>
<h2 id="Algorithm-Application-in-Sklearn"><a href="#Algorithm-Application-in-Sklearn" class="headerlink" title="Algorithm Application in Sklearn"></a>Algorithm Application in Sklearn</h2><h3 id="对鸢尾花-Iris-进行分类"><a href="#对鸢尾花-Iris-进行分类" class="headerlink" title="对鸢尾花(Iris)进行分类"></a>对鸢尾花(Iris)进行分类</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier,export_graphviz</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">decision_iris</span>():</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    用决策树对鸢尾花分类</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">#1.获取数据集</span></span><br><span class="line">    iris = load_iris()</span><br><span class="line">    <span class="comment">#2.划分数据集</span></span><br><span class="line">    x_train,x_test,y_train,y_test = train_test_split(iris.data,iris.target,random_state=<span class="number">22</span>)</span><br><span class="line">    <span class="comment">#3.决策树预估器</span></span><br><span class="line">    estimator = DecisionTreeClassifier(criterion=<span class="string">"entropy"</span>)</span><br><span class="line">    estimator.fit(x_train,y_train)</span><br><span class="line">    <span class="comment">#4.模型评估</span></span><br><span class="line">    <span class="comment">#方法1:直接对比真实值和预测值</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"y_predict:\n"</span>,y_predict)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"直接对比真实值和预测值:\n"</span>,y_test==y_predict)</span><br><span class="line">    <span class="comment">#方法2:计算准确率</span></span><br><span class="line">    score = estimator.score(x_test,y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"准确率为:\n"</span>,score)</span><br><span class="line">    <span class="comment">#5.可视化决策树</span></span><br><span class="line">    export_graphviz(estimator,out_file=<span class="string">"iris_tree.dot"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    decision_iris()</span><br></pre></td></tr></table></figure>
<p><strong>结果：</strong></p>
<p><img src="/2022/08/21/Decision-Tree/iris.png" alt></p>
<p><strong>可视化：dot文件</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">digraph Tree {</span><br><span class="line">node [shape=box, fontname=<span class="string">"helvetica"</span>] ;</span><br><span class="line">edge [fontname=<span class="string">"helvetica"</span>] ;</span><br><span class="line"><span class="number">0</span> [label=<span class="string">"X[2] &lt;= 2.45\nentropy = 1.584\nsamples = 112\nvalue = [39, 37, 36]"</span>] ;</span><br><span class="line"><span class="number">1</span> [label=<span class="string">"entropy = 0.0\nsamples = 39\nvalue = [39, 0, 0]"</span>] ;</span><br><span class="line"><span class="number">0</span> -&gt; <span class="number">1</span> [labeldistance=<span class="number">2.5</span>, labelangle=<span class="number">45</span>, headlabel=<span class="string">"True"</span>] ;</span><br><span class="line"><span class="number">2</span> [label=<span class="string">"X[3] &lt;= 1.75\nentropy = 1.0\nsamples = 73\nvalue = [0, 37, 36]"</span>] ;</span><br><span class="line"><span class="number">0</span> -&gt; <span class="number">2</span> [labeldistance=<span class="number">2.5</span>, labelangle=-<span class="number">45</span>, headlabel=<span class="string">"False"</span>] ;</span><br><span class="line"><span class="number">3</span> [label=<span class="string">"X[2] &lt;= 5.05\nentropy = 0.391\nsamples = 39\nvalue = [0, 36, 3]"</span>] ;</span><br><span class="line"><span class="number">2</span> -&gt; <span class="number">3</span> ;</span><br><span class="line"><span class="number">4</span> [label=<span class="string">"X[0] &lt;= 4.95\nentropy = 0.183\nsamples = 36\nvalue = [0, 35, 1]"</span>] ;</span><br><span class="line"><span class="number">3</span> -&gt; <span class="number">4</span> ;</span><br><span class="line"><span class="number">5</span> [label=<span class="string">"X[3] &lt;= 1.35\nentropy = 1.0\nsamples = 2\nvalue = [0, 1, 1]"</span>] ;</span><br><span class="line"><span class="number">4</span> -&gt; <span class="number">5</span> ;</span><br><span class="line"><span class="number">6</span> [label=<span class="string">"entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"</span>] ;</span><br><span class="line"><span class="number">5</span> -&gt; <span class="number">6</span> ;</span><br><span class="line"><span class="number">7</span> [label=<span class="string">"entropy = 0.0\nsamples = 1\nvalue = [0, 0, 1]"</span>] ;</span><br><span class="line"><span class="number">5</span> -&gt; <span class="number">7</span> ;</span><br><span class="line"><span class="number">8</span> [label=<span class="string">"entropy = 0.0\nsamples = 34\nvalue = [0, 34, 0]"</span>] ;</span><br><span class="line"><span class="number">4</span> -&gt; <span class="number">8</span> ;</span><br><span class="line"><span class="number">9</span> [label=<span class="string">"X[0] &lt;= 6.05\nentropy = 0.918\nsamples = 3\nvalue = [0, 1, 2]"</span>] ;</span><br><span class="line"><span class="number">3</span> -&gt; <span class="number">9</span> ;</span><br><span class="line"><span class="number">10</span> [label=<span class="string">"entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"</span>] ;</span><br><span class="line"><span class="number">9</span> -&gt; <span class="number">10</span> ;</span><br><span class="line"><span class="number">11</span> [label=<span class="string">"entropy = 0.0\nsamples = 2\nvalue = [0, 0, 2]"</span>] ;</span><br><span class="line"><span class="number">9</span> -&gt; <span class="number">11</span> ;</span><br><span class="line"><span class="number">12</span> [label=<span class="string">"X[2] &lt;= 4.85\nentropy = 0.191\nsamples = 34\nvalue = [0, 1, 33]"</span>] ;</span><br><span class="line"><span class="number">2</span> -&gt; <span class="number">12</span> ;</span><br><span class="line"><span class="number">13</span> [label=<span class="string">"entropy = 0.0\nsamples = 1\nvalue = [0, 1, 0]"</span>] ;</span><br><span class="line"><span class="number">12</span> -&gt; <span class="number">13</span> ;</span><br><span class="line"><span class="number">14</span> [label=<span class="string">"entropy = 0.0\nsamples = 33\nvalue = [0, 0, 33]"</span>] ;</span><br><span class="line"><span class="number">12</span> -&gt; <span class="number">14</span> ;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>
<p><strong>然后把这个dot文件内容拉到</strong><a target="_blank" rel="noopener" href="http://www.webgraphviz.com/">这个网站里面</a>，或者用vscode里面的插件“Graphviz (dot) language support for Visual Studio Code”</p>
<p><img src="/2022/08/21/Decision-Tree/dt.png" alt></p>
<h3 id="泰坦尼克号乘客分类案例实现"><a href="#泰坦尼克号乘客分类案例实现" class="headerlink" title="泰坦尼克号乘客分类案例实现"></a>泰坦尼克号乘客分类案例实现</h3><h4 id="流程分析"><a href="#流程分析" class="headerlink" title="流程分析"></a>流程分析</h4><p>①获取数据</p>
<p>②数据处理</p>
<p>​    📴缺失值处理：特征值-&gt;字典类型</p>
<p>③准备好特征值 ，目标值</p>
<p>④划分数据集</p>
<p>⑤特征工程</p>
<p>⑥决策树预估</p>
<p>⑦模型评估</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier, export_graphviz</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>():</span><br><span class="line">    data = pd.read_csv(<span class="string">"../../titanic.csv"</span>)</span><br><span class="line">    titanic = data.copy()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 方法一: 过滤掉空的值的数据组, 准确率高点</span></span><br><span class="line">    data_used = titanic[[<span class="string">"pclass"</span>, <span class="string">"age"</span>, <span class="string">"sex"</span>, <span class="string">"survived"</span>]]</span><br><span class="line">    real_data = pd.DataFrame(columns=[<span class="string">"pclass"</span>, <span class="string">"age"</span>, <span class="string">"sex"</span>, <span class="string">"survived"</span>])</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> data_used.values:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> np.isnan(row[<span class="number">1</span>]):</span><br><span class="line">            real_data = real_data.append([{<span class="string">'pclass'</span>: row[<span class="number">0</span>], <span class="string">'age'</span>: row[<span class="number">1</span>],</span><br><span class="line">                                           <span class="string">'sex'</span>: row[<span class="number">2</span>], <span class="string">'survived'</span>: row[<span class="number">3</span>]}],</span><br><span class="line">                                         ignore_index=<span class="literal">True</span>)</span><br><span class="line">    x = real_data[[<span class="string">"pclass"</span>, <span class="string">"age"</span>, <span class="string">"sex"</span>]].to_dict(orient=<span class="string">"records"</span>)</span><br><span class="line">    y = real_data[<span class="string">"survived"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 方法二: 对空数据设置个非0值</span></span><br><span class="line">    <span class="comment"># x = titanic[["pclass", "age", "sex"]]  # 只提取这一些特征</span></span><br><span class="line">    <span class="comment"># y = titanic["survived"]  # 目标值</span></span><br><span class="line">    <span class="comment"># x["age"].fillna(x["age"].mean(), inplace=True)</span></span><br><span class="line">    <span class="comment"># x = x.to_dict(orient="records")</span></span><br><span class="line"></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y.astype(<span class="string">'int'</span>), random_state=<span class="number">22</span>)</span><br><span class="line">    <span class="keyword">return</span> x_train, x_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_tree</span>(<span class="params">estimator, feature_name</span>):</span><br><span class="line">    export_graphviz(estimator, out_file=<span class="string">"../titanic_tree.dot"</span>, feature_names=feature_name)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">titanic_test</span>():</span><br><span class="line">    x_train, x_test, y_train, y_test = load_data()</span><br><span class="line"></span><br><span class="line">    transfer = DictVectorizer()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line">    estimator = DecisionTreeClassifier(criterion=<span class="string">"entropy"</span>, max_depth=<span class="number">12</span>)</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    show_tree(estimator, transfer.get_feature_names())</span><br><span class="line"></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"预测值为:"</span>, y_predict, <span class="string">"\n真实值为:"</span>, y_test, <span class="string">"\n比较结果为:"</span>, y_test == y_predict)</span><br><span class="line">    score = estimator.score(x_test, y_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"准确率为: "</span>, score)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    titanic_test()</span><br></pre></td></tr></table></figure>
<h2 id="Ensemble-Learning"><a href="#Ensemble-Learning" class="headerlink" title="Ensemble Learning"></a>Ensemble Learning</h2><p>“三个臭皮匠，顶个诸葛亮”：<strong>没有创造出新的算法，</strong>而是把已有的算法进行结合，从而得到更好的效果。</p>
<p>在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的并且各个方面都表现很好的模型，<strong>集成学习</strong>是组合这些多个弱监督模型来得到更好更全面的强监督模型（理解为组合金刚）</p>
<p><img src="/2022/08/21/Decision-Tree/ba.png" alt></p>
<p>集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。在周志华西瓜书中通过Hoeffding不等式证明了，<strong>随着集成中个体分类器数目的增多</strong>，<strong>集成的错误率将呈指数级下降</strong>，<strong>最终趋于零</strong>。</p>
<ul>
<li><strong>数据集大：划分成多个小数据集，学习多个模型进行组合
  </strong></li>
<li><strong>数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合</strong></li>
</ul>
<p>分类：</p>
<p>①Boosting：各学习器个体之间强依赖<strong>==必须串行==</strong></p>
<p>②Bagging：各学习器个体之间<strong>不存在</strong>强依赖<strong>==可以并行==</strong></p>
<p>③Stacking：单独分类</p>
<p>其中①②较为常见。</p>
<h3 id="Boosting：经典串行集成学习方法"><a href="#Boosting：经典串行集成学习方法" class="headerlink" title="Boosting：经典串行集成学习方法"></a>Boosting：经典串行集成学习方法</h3><p><del>请脑补计算机组成与体系结构中串行进位加法器的图</del></p>
<p>核心思想：挑选精英</p>
<p>基模型按次序逐个进行训练，基模型的训练集按照某种策略每次都进行一定的转化。对所有基模型预测的结果进行线性综合产生最终的预测结果。大部分情况下，<strong>经过 Boosting 得到的结果偏差（bias）更小</strong>。</p>
<p><img src="/2022/08/21/Decision-Tree/22.png" alt></p>
<h4 id="AdaBoost-Adaptive-Boosting"><a href="#AdaBoost-Adaptive-Boosting" class="headerlink" title="AdaBoost(Adaptive Boosting)"></a>AdaBoost(Adaptive Boosting)</h4><p><img src="/2022/08/21/Decision-Tree/adab.png" alt></p>
<p>具体说来，算法3步走：</p>
<p><strong>①初始化训练数据的权值分布。</strong>如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。</p>
<p><strong>②训练弱分类器。</strong>具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</p>
<p><strong>③将各个训练得到的弱分类器组合成强分类器。</strong>各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。</p>
<p><strong>只可用于处理二分类任务</strong></p>
<p>🤡Q：二分类是什么？？</p>
<p>🧐A：分两类，比如垃圾邮件检测、用户流失等。</p>
<p><img src="/2022/08/21/Decision-Tree/AB.png" alt></p>
<p>从偏差-方差分解的角度看，Boosting主要关注<strong>降低偏差</strong>，因此Boosting基于泛化性能相当弱的学习器可以构建出很强的集成。</p>
<h5 id="代码前置知识"><a href="#代码前置知识" class="headerlink" title="代码前置知识"></a><strong>代码前置知识</strong></h5><h6 id="①iloc函数"><a href="#①iloc函数" class="headerlink" title="①iloc函数"></a>①iloc函数</h6><p>属于pandas库，全称“index location”</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iloc[:,:]</span><br></pre></td></tr></table></figure>
<p>👆左侧冒号表示行  右侧冒号表示列</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">data.iloc[<span class="number">0</span>] <span class="comment">#得到属性名、第一行数据、数据类型</span></span><br><span class="line">data.iloc[<span class="number">1</span>] <span class="comment">#得到属性名、第二行数据、数据类型</span></span><br><span class="line">data.iloc[:] / data.iloc[<span class="number">0</span>:] / data.iloc[:,:] <span class="comment">#得到全部数据</span></span><br><span class="line">data.iloc[<span class="number">1</span>:] <span class="comment">#得到第二行开始的数据</span></span><br><span class="line">data.iloc[<span class="number">2</span>:, <span class="number">3</span>:] <span class="comment">#得到第3-n行，第4-m列的数据（假设共有n行，m列）</span></span><br></pre></td></tr></table></figure>
<p>在本案例中的代码解释在<strong>对应注释</strong>里面</p>
<h6 id="②sklearn的转换库"><a href="#②sklearn的转换库" class="headerlink" title="②sklearn的转换库"></a>②sklearn的转换库</h6><p>作用：清洗，降维，提取特征等。</p>
<p>数据转换中有3种很重要的方法：<code>fit</code>,<code>fit_transform</code>,<code>transform</code></p>
<p>fit:</p>
<p>​    求得训练集X的均值，方差，最大值，最小值这些训练集X固有的属性。（入门级）</p>
<p>transform:</p>
<p>​    在fit的基础上，进行标准化，降维，归一化等操作。</p>
<p>fit_transform:</p>
<p>​    <u>“joins the fit() and transform() method for transformation of dataset.”</u></p>
<p>​    很高效的将模型训练和转化合并到一起，训练样本先做fit，得到mean（均值），standard deviation（标准差），然后将这些参数用于transform（归一化训练数据），使得到的训练数据是归一化的。</p>
<p>注意🖐</p>
<p>·运行结果一模一样不代表这两个函数可以互相替换，绝对不可以！！！（目前我还没有遇到类似的问题，毕竟刚刚接触ML）</p>
<p>解释👱‍♂️</p>
<p>·sklearn里的封装好的各种算法都要fit、然后调用各种API方法，transform只是其中一个API方法，所以当你调用除transform之外的方法，必须要先fit，为了通用的写代码，还是分开写比较好</p>
<p>·也就是说，这个fit相对于transform而言是没有任何意义的，但是相对于整个代码而言，fit是为后续的API函数服务的，所以fit_transform不能改写为transform。</p>
<h6 id="③np-arange"><a href="#③np-arange" class="headerlink" title="③np.arange()"></a>③np.arange()</h6><p>函数<strong>返回</strong>一个有终点和起点的固定步长的排列，如[1,2,3,4,5]，起点是1，终点是6，步长为1。</p>
<p><strong>参数：</strong></p>
<p>·1个参数：参数值为终点，步长默认取1</p>
<p>·2个参数：起点——&gt;终点，步长默认取1</p>
<p>·3个参数：起点——&gt;终点，步长任意，支持小数</p>
<h6 id="④np-meshgrid"><a href="#④np-meshgrid" class="headerlink" title="④np.meshgrid()"></a>④np.meshgrid()</h6><p>👉<a target="_blank" rel="noopener" href="https://blog.csdn.net/lllxxq141592654/article/details/81532855">讲解</a></p>
<h5 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">iris = sns.load_dataset(<span class="string">"iris"</span>)</span><br><span class="line">x = iris.iloc[:<span class="number">100</span>].iloc[:,[<span class="number">0</span>,<span class="number">1</span>]] <span class="comment">#100行，前两列</span></span><br><span class="line">y = iris.iloc[:<span class="number">100</span>].iloc[:,-<span class="number">1</span>] <span class="comment">#100行，倒数第一列</span></span><br><span class="line">encoder = LabelEncoder()<span class="comment">#对数据集进行编码</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#转换库：可以清洗，降维，提取特征</span></span><br><span class="line">y = encoder.fit_transform(y) <span class="comment">#将标签设置为0，1, 2样式 </span></span><br><span class="line">x = np.array(x) <span class="comment">#转换为坐标矩阵形式</span></span><br><span class="line">y = np.array(y) <span class="comment">#转换为矩阵形式</span></span><br><span class="line">plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],c=y)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#设定弱分类器CART，每个树的最大深度设置为2</span></span><br><span class="line">weakClassifier = DecisionTreeClassifier(max_depth=<span class="number">2</span>)</span><br><span class="line">clf = AdaBoostClassifier(base_estimator=weakClassifier,algorithm=<span class="string">'SAMME'</span>,n_estimators=<span class="number">300</span>,learning_rate=<span class="number">0.8</span>)</span><br><span class="line">clf.fit(x,y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#生成测试数据</span></span><br><span class="line">x1_min = x[:,<span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span></span><br><span class="line">x1_max = x[:,<span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">x2_min = x[:,<span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span></span><br><span class="line">x2_max = x[:,<span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">x1_ , x2_ = np.meshgrid(np.arange(x1_min,x1_max,<span class="number">0.02</span>),np.arange(x2_min,x2_max,<span class="number">0.02</span>))</span><br><span class="line"></span><br><span class="line">y_=clf.predict(np.c_[x1_.ravel(),x2_.ravel()]) <span class="comment"># ravel函数将二维数组扁平化为一维</span></span><br><span class="line">y_=y_.reshape(x1_.shape)</span><br><span class="line">plt.contourf(x1_,x2_,y_,cmap=plt.cm.Paired)</span><br><span class="line">plt.scatter(x[:,<span class="number">0</span>],x[:,<span class="number">1</span>],c=y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2022/08/21/Decision-Tree/F2.png" alt></p>
<p><img src="/2022/08/21/Decision-Tree/F1.png" alt></p>
<h3 id="Bagging：经典并行集成学习方法"><a href="#Bagging：经典并行集成学习方法" class="headerlink" title="Bagging：经典并行集成学习方法"></a>Bagging：经典并行集成学习方法</h3><p><img src="/2022/08/21/Decision-Tree/boost.png" alt></p>
<p>具体步骤如下：</p>
<ul>
<li><strong>采用重抽样方法（有放回抽样）从原始样本中抽取一定数量的样本</strong></li>
<li><strong>根据抽出的样本计算想要得到的统计量M</strong></li>
<li><strong>重复上述T次（一般大于1000），得到T个统计量T</strong></li>
<li><strong>根据这T个统计量，即可计算出统计量的置信区间</strong></li>
</ul>
<h4 id="Eg-Random-Forest"><a href="#Eg-Random-Forest" class="headerlink" title="Eg:Random Forest"></a>Eg:Random Forest</h4><p>·包含多个决策树的分类器。🌲+🌲+🌲</p>
<p>eg：4个🌲是true，1个🌲是false，那么结果就是true</p>
<p>假设训练集有N个样本，M个特征</p>
<p>·Random体现&amp;建造每棵树的算法：</p>
<p>​    训练集随机：BootStrap抽样（随机有放回抽样 ），N个样本随机有放回抽样N个。</p>
<p>​    特征随机：从M个特征选m个特征（M&gt;&gt;m：降维 ）</p>
<p>这样一来，正确的树是互相吻合的，错误的数会错的各不相同。</p>
<h5 id="Advantages"><a href="#Advantages" class="headerlink" title="Advantages"></a>Advantages</h5><p>①训练可以高度并行化，可以有效运行在大数据集上。</p>
<p>②对部分特征的缺失容忍度高。</p>
<p>③由于有了样本和属性的采样，最终训练出来的模型泛化能力强。</p>
<p>✍泛化能力(generalization ability)：ML算法对新样本的适应能力.</p>
<h5 id="Disadvantages"><a href="#Disadvantages" class="headerlink" title="Disadvantages"></a>Disadvantages</h5><p>①在某些噪声比较大的样本集上，随机森林容易陷入过拟合。</p>
<p>②取值划分比较多的特征容易对随机森林的决策产生更大的影响，从而影响拟合的模型效果。</p>
<h4 id="👇Example"><a href="#👇Example" class="headerlink" title="👇Example"></a>👇Example</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"># 随机森林就是多个树, 最后通过投票选择多数的那个决策</span></span><br><span class="line"><span class="string"># 随机有两种方式</span></span><br><span class="line"><span class="string"># 1: 每一个树训练集不同</span></span><br><span class="line"><span class="string"># 2: 需要训练的特征进行随机分配 从特定的特征集里面抽取一些特征来分配</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_data</span>():</span><br><span class="line">    data = pd.read_csv(<span class="string">"../../titanic.csv"</span>)</span><br><span class="line">    titanic = data.copy()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 方法一: 过滤掉空的值的数据组, 准确率高点</span></span><br><span class="line">    data_used = titanic[[<span class="string">"pclass"</span>, <span class="string">"age"</span>, <span class="string">"sex"</span>, <span class="string">"survived"</span>]]</span><br><span class="line">    real_data = pd.DataFrame(columns=[<span class="string">"pclass"</span>, <span class="string">"age"</span>, <span class="string">"sex"</span>, <span class="string">"survived"</span>])</span><br><span class="line">    <span class="keyword">for</span> row <span class="keyword">in</span> data_used.values:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> np.isnan(row[<span class="number">1</span>]):</span><br><span class="line">            real_data = real_data.append([{<span class="string">'pclass'</span>: row[<span class="number">0</span>], <span class="string">'age'</span>: row[<span class="number">1</span>],</span><br><span class="line">                                           <span class="string">'sex'</span>: row[<span class="number">2</span>], <span class="string">'survived'</span>: row[<span class="number">3</span>]}],</span><br><span class="line">                                         ignore_index=<span class="literal">True</span>)</span><br><span class="line">    x = real_data[[<span class="string">"pclass"</span>, <span class="string">"age"</span>, <span class="string">"sex"</span>]].to_dict(orient=<span class="string">"records"</span>)</span><br><span class="line">    y = real_data[<span class="string">"survived"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 方法二: 对空数据设置个非0值</span></span><br><span class="line">    <span class="comment"># x = titanic[["pclass", "age", "sex"]]  # 只提取这一些特征</span></span><br><span class="line">    <span class="comment"># y = titanic["survived"]  # 目标值</span></span><br><span class="line">    <span class="comment"># x["age"].fillna(x["age"].mean(), inplace=True)</span></span><br><span class="line">    <span class="comment"># x = x.to_dict(orient="records")</span></span><br><span class="line"></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(x, y.astype(<span class="string">'int'</span>), random_state=<span class="number">22</span>)</span><br><span class="line">    <span class="keyword">return</span> x_train, x_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">titanic_ramdo_test</span>():</span><br><span class="line">    x_train, x_test, y_train, y_test = load_data()</span><br><span class="line"></span><br><span class="line">    transfer = DictVectorizer()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line">    estimator = RandomForestClassifier()</span><br><span class="line">    <span class="comment"># 默认bootstrap 表示为true,也就是说默认情况下放回抽样</span></span><br><span class="line"></span><br><span class="line">    param_dict = {<span class="string">"n_estimators"</span>: [<span class="number">120</span>, <span class="number">200</span>, <span class="number">300</span>, <span class="number">500</span>, <span class="number">800</span>, <span class="number">1200</span>],</span><br><span class="line">                  <span class="string">"max_depth"</span>: [<span class="number">5</span>, <span class="number">8</span>, <span class="number">15</span>, <span class="number">25</span>, <span class="number">30</span>]}</span><br><span class="line">    estimator = GridSearchCV(estimator, param_grid=param_dict, cv=<span class="number">3</span>)</span><br><span class="line">    estimator.fit(x_train, y_train)  <span class="comment"># 训练集里面的数据和目标值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 传入测试值通过前面的预估器获得预测值</span></span><br><span class="line">    y_predict = estimator.predict(x_test)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"预测值为:"</span>, y_predict, <span class="string">"\n真实值为:"</span>, y_test, <span class="string">"\n比较结果为:"</span>, y_test == y_predict)</span><br><span class="line">    score = estimator.score(x_train, y_train)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"准确率为: "</span>, score)</span><br><span class="line">    <span class="comment"># ------------------</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"最佳参数:\n"</span>, estimator.best_params_)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"最佳结果:\n"</span>, estimator.best_score_)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"最佳估计器:\n"</span>, estimator.best_estimator_)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"交叉验证结果:\n"</span>, estimator.cv_results_)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    titanic_ramdo_test()</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://nexuslbh.top">NexusLbh</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://nexuslbh.top/2022/08/21/Decision-Tree/">http://nexuslbh.top/2022/08/21/Decision-Tree/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://nexuslbh.top" target="_blank">Meta Field</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ML/">ML</a></div><div class="post_share"><div class="social-share" data-image="/img/men.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/08/21/MYSQL-LeetcodePractice-DAY05&amp;06-%E5%90%88%E5%B9%B6/" title="MYSQL-LeetcodePractice-DAY05&amp;06-合并"><img class="cover" src="https://w.wallhaven.cc/full/kx/wallhaven-kx79w6.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">MYSQL-LeetcodePractice-DAY05&amp;06-合并</div></div></a></div><div class="next-post pull-right"><a href="/2022/08/20/MYSQL-LeetcodePractice-DAY04-%E7%BB%84%E5%90%88%E6%9F%A5%E8%AF%A2-%E6%8C%87%E5%AE%9A%E9%80%89%E5%8F%96/" title="MYSQL-LeetcodePractice-DAY04-组合查询&amp;指定选取"><img class="cover" src="https://w.wallhaven.cc/full/o5/wallhaven-o532dp.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">MYSQL-LeetcodePractice-DAY04-组合查询&amp;指定选取</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/08/31/GBDT/" title="GBDT"><img class="cover" src="/img/cloud.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-31</div><div class="title">GBDT</div></div></a></div><div><a href="/2022/08/19/Genetic-Algorithm/" title="Genetic Algorithm"><img class="cover" src="https://w.wallhaven.cc/full/kx/wallhaven-kx79w6.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-19</div><div class="title">Genetic Algorithm</div></div></a></div><div><a href="/2022/08/26/KNN/" title="KNN"><img class="cover" src="/img/tq.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-26</div><div class="title">KNN</div></div></a></div><div><a href="/2022/08/24/Machine-Learning-Base/" title="Machine Learning Base"><img class="cover" src="/img/ms3.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-24</div><div class="title">Machine Learning Base</div></div></a></div><div><a href="/2022/08/26/SVM/" title="SVM"><img class="cover" src="https://w.wallhaven.cc/full/2y/wallhaven-2y8x99.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-26</div><div class="title">SVM</div></div></a></div><div><a href="/2022/08/29/Neural-Network/" title="Neural Network"><img class="cover" src="/img/jp.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-08-29</div><div class="title">Neural Network</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">NexusLbh</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">67</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/NexusXDlbh"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/NexusXDlbh" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:3321861647@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Basic-Theories"><span class="toc-number">1.</span> <span class="toc-text">Basic Theories</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Algorithm-Priciple"><span class="toc-number">1.1.</span> <span class="toc-text">1.Algorithm Priciple</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Information-Entropy-amp-Information-Gain"><span class="toc-number">1.2.</span> <span class="toc-text">2.Information Entropy &amp; Information Gain</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Algorithm-Implementation-in-Python"><span class="toc-number">2.</span> <span class="toc-text">Algorithm Implementation in Python</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Core-Problems"><span class="toc-number">2.1.</span> <span class="toc-text">1.Core Problems</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Implementation"><span class="toc-number">2.2.</span> <span class="toc-text">2.Implementation</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9"><span class="toc-number">2.2.1.</span> <span class="toc-text">(1)特征选择</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%86%B3%E7%AD%96%E6%A0%91%E7%94%9F%E6%88%90%EF%BC%88%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91%EF%BC%89"><span class="toc-number">2.2.2.</span> <span class="toc-text">(2)决策树生成（构建决策树）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%98%E5%82%A8"><span class="toc-number">2.2.3.</span> <span class="toc-text">(3)决策树存储</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB"><span class="toc-number">2.2.4.</span> <span class="toc-text">(4)决策树分类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E5%86%B3%E7%AD%96%E6%A0%91%E7%BB%98%E5%88%B6"><span class="toc-number">2.2.5.</span> <span class="toc-text">(5)决策树绘制</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#6-%E5%86%B3%E7%AD%96%E6%A0%91%E5%89%AA%E6%9E%9D%EF%BC%88%E4%BA%86%E8%A7%A3%EF%BC%89"><span class="toc-number">2.2.6.</span> <span class="toc-text">(6)决策树剪枝（了解）</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Algorithm-Application-in-Sklearn"><span class="toc-number">3.</span> <span class="toc-text">Algorithm Application in Sklearn</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E9%B8%A2%E5%B0%BE%E8%8A%B1-Iris-%E8%BF%9B%E8%A1%8C%E5%88%86%E7%B1%BB"><span class="toc-number">3.1.</span> <span class="toc-text">对鸢尾花(Iris)进行分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E4%B9%98%E5%AE%A2%E5%88%86%E7%B1%BB%E6%A1%88%E4%BE%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.2.</span> <span class="toc-text">泰坦尼克号乘客分类案例实现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90"><span class="toc-number">3.2.1.</span> <span class="toc-text">流程分析</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">3.3.</span> <span class="toc-text">代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ensemble-Learning"><span class="toc-number">4.</span> <span class="toc-text">Ensemble Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Boosting%EF%BC%9A%E7%BB%8F%E5%85%B8%E4%B8%B2%E8%A1%8C%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="toc-number">4.1.</span> <span class="toc-text">Boosting：经典串行集成学习方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#AdaBoost-Adaptive-Boosting"><span class="toc-number">4.1.1.</span> <span class="toc-text">AdaBoost(Adaptive Boosting)</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="toc-number">4.1.1.1.</span> <span class="toc-text">代码前置知识</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E2%91%A0iloc%E5%87%BD%E6%95%B0"><span class="toc-number">4.1.1.1.1.</span> <span class="toc-text">①iloc函数</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E2%91%A1sklearn%E7%9A%84%E8%BD%AC%E6%8D%A2%E5%BA%93"><span class="toc-number">4.1.1.1.2.</span> <span class="toc-text">②sklearn的转换库</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E2%91%A2np-arange"><span class="toc-number">4.1.1.1.3.</span> <span class="toc-text">③np.arange()</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E2%91%A3np-meshgrid"><span class="toc-number">4.1.1.1.4.</span> <span class="toc-text">④np.meshgrid()</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Code"><span class="toc-number">4.1.1.2.</span> <span class="toc-text">Code</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Bagging%EF%BC%9A%E7%BB%8F%E5%85%B8%E5%B9%B6%E8%A1%8C%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95"><span class="toc-number">4.2.</span> <span class="toc-text">Bagging：经典并行集成学习方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Eg-Random-Forest"><span class="toc-number">4.2.1.</span> <span class="toc-text">Eg:Random Forest</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Advantages"><span class="toc-number">4.2.1.1.</span> <span class="toc-text">Advantages</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Disadvantages"><span class="toc-number">4.2.1.2.</span> <span class="toc-text">Disadvantages</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%91%87Example"><span class="toc-number">4.2.2.</span> <span class="toc-text">👇Example</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/05/19/Crypto-lab/" title="Crypto_lab"><img src="/img/tree.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Crypto_lab"/></a><div class="content"><a class="title" href="/2023/05/19/Crypto-lab/" title="Crypto_lab">Crypto_lab</a><time datetime="2023-05-19T08:18:07.270Z" title="更新于 2023-05-19 16:18:07">2023-05-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/11/05/DES/" title="DES"><img src="/img/tree.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="DES"/></a><div class="content"><a class="title" href="/2022/11/05/DES/" title="DES">DES</a><time datetime="2023-05-19T07:48:14.964Z" title="更新于 2023-05-19 15:48:14">2023-05-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/03/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E5%AE%9E%E9%AA%8C%E4%B8%89/" title="大数据隐私保护实验三"><img src="/img/tree.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大数据隐私保护实验三"/></a><div class="content"><a class="title" href="/2023/03/01/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E5%AE%9E%E9%AA%8C%E4%B8%89/" title="大数据隐私保护实验三">大数据隐私保护实验三</a><time datetime="2023-05-11T11:01:11.771Z" title="更新于 2023-05-11 19:01:11">2023-05-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E9%99%84%E5%8A%A0%E5%AE%9E%E9%AA%8C/" title="大数据隐私保护附加实验"><img src="/img/niaokan.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="大数据隐私保护附加实验"/></a><div class="content"><a class="title" href="/2023/04/06/%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9A%90%E7%A7%81%E4%BF%9D%E6%8A%A4%E9%99%84%E5%8A%A0%E5%AE%9E%E9%AA%8C/" title="大数据隐私保护附加实验">大数据隐私保护附加实验</a><time datetime="2023-04-17T13:13:35.563Z" title="更新于 2023-04-17 21:13:35">2023-04-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/04/03/chatgpt-academic/" title="chatgpt_academic"><img src="/img/tree.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="chatgpt_academic"/></a><div class="content"><a class="title" href="/2023/04/03/chatgpt-academic/" title="chatgpt_academic">chatgpt_academic</a><time datetime="2023-04-07T03:54:49.444Z" title="更新于 2023-04-07 11:54:49">2023-04-07</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/men.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By NexusLbh</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? 'dark' : 'default'

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script></div><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = true;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false});</script></body></html>